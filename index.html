<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="Files/jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Junlin Han</title>
</head>
<body>

<a id="home" class="anchor"></a>
<div id="container">
<div class="container">
<table class="imgtable"><tr><td>
<a><img src="./Files/face.jpg" alt="" align="left" height="200px"/></a></td>
<td align="left"><p><font size="4"> <b> Junlin (Hans) Han </b> </font><font size="4"; font style="font-family:Microsoft YaHei"></font><font size="4"></font><br />
<a href="https://ai.meta.com/genai/">GenAI</a>, <a href="https://about.meta.com/"> Meta </a></br >
<a href="https://torrvision.com/index.html"> Torr Vision Group </a>, <a href="https://www.ox.ac.uk/"> University of Oxford </a></br>
<br />Location: Meta Kings Cross, London N1C 4DB  <br>
<class="staffshortcut">
 <A HREF="#Profile">Profile</A> |
<A HREF="#News">News</A> |
 <A HREF="#Interest">Research Interests</A> |
 <A HREF="#Education">Education</A> |
  <A HREF="#Education">Experience </A> |
 <A HREF="#Publications">Publications</A> |
 <A HREF="#Services">Services</A> |
 <!-- <A HREF="#Skills">Skills</A> | -->
 <A HREF="#Awards">Awards</A>|
  <A HREF="#Miscs">Miscs</A>
<br />
Email: junlinhcv@gmail.com; junlinhan@meta.com; junlin.han@eng.ox.ac.uk<br />
[<a href="https://github.com/junlinhan" target="_blank">GitHub</a>]
[<a href="./Files/CV.pdf">Curriculum Vitae</a>]
[<a href="https://scholar.google.com/citations?user=5L0Uj_IAAAAJ&hl=en&authuser=1&oi=ao" target="_blank">Google Scholar</a>]
[<a href="https://twitter.com/han_junlin" target="_blank">Twitter/X</a>]
</td></tr></table>





<A NAME="Profile"><h2>Profile</h2></A>
<b>Greetings, welcome to my website!</b>
<ul>
  <li>I am a researcher at <a href="https://ai.meta.com/genai/">Meta Superintelligence Labs</a>, and a second-year PhD (DPhil) student at the <a href="https://torrvision.com/index.html">Torr Vision Group</a>, <a href="https://www.ox.ac.uk/">University of Oxford</a>, working with <a href="https://en.wikipedia.org/wiki/Philip_Torr">Prof. Philip Torr</a>. 
    Before that, I worked as a research scientist at <a href="https://www.cybever.ai/">Cybever</a>, a 3D AIGC startup for a few months. 
    I obtained my BSc from the <a href="https://www.anu.edu.au/">Australian National University</a> under the supervision of <a href="https://people.csiro.au/P/L/Lars-Petersson">Dr. Lars Petersson</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>, and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>.</li>

<li>I acknowledge that information asymmetry can significantly hinder research opportunities for junior students. If you’re interested in chatting about life, research, or potential collaborations, feel free to email me. </li>
<li> For undergrad and master's students at Oxford ONLY: If you are interested in an internship or 4YP at TVG working with me and Phil, please email me your current CV to discuss further. </li>
</ul>


      
<A NAME="News"><h2>News</h2></A>
<ul id="news-list">
  <li>[09.2025] <a href="https://arxiv.org/abs/2506.07227">Hallucination at a Glance</a> was accepted to NeurIPS 2025, a new dataset for benchmarking and training MLLMs for fine-grained visual reasoning!  </li>
  <li>[07.2025] <a href="https://dl.acm.org/doi/full/10.1145/3721238.3730704">3D2EP</a> was accepted to SIGGRAPH 2025, a new method for 3D shape decomposition that represents objects as parametric primitives. </li>
  <li>[05.2025] <a href="https://junlinhan.github.io/projects/flex3d">Flex3D</a> was accepted to ICML 2025, a new two-stage pipeline for high-quality 3D generation and reconstruction!</li>
  <li>[03.2025] One paper <a href="https://arxiv.org/abs/2503.16282">on point-cloud segmentation</a> was accepted to CVPR 2025, congrats to <a href="https://zhaochongan.github.io/">Zhaochong</a>!</li>
  <li>[12.2024] Our previous <a href="https://www.mdpi.com/2072-4292/14/17/4297">work on low-level vision</a> won the best paper award in Remote Sensing!</li>
  <li class="news-item-hidden" >[12.2024] We will be organizing the <a href="https://fm-wild-community.github.io/">2nd Workshop on Foundation Models in the Wild</a> at ICLR 2025!</li> 
  <li class="news-item-hidden">[11.2024] Two papers (<a href="https://chuny1.github.io/3DGPT/3dgpt.html">3D-GPT</a> and <a href="https://dreambeast3d.github.io/">DreamBeast</a>) were accepted to <a href="https://3dvconf.github.io/2025/">3DV 2025</a>. Congratulations to <a href="https://chuny1.github.io/">Chunyi</a> and <a href="https://runjiali-rl.github.io/">Runjia</a>! 3D-GPT is pioneering work in large-scale 3D scene generation, and DreamBeast is a fascinating project generating fantastical 3D animals.</li>
  <li class="news-item-hidden">[07.2024] <a href="https://junlinhan.github.io/projects/vfusion3d.html">VFusion3D</a> and <a href="https://arxiv.org/abs/2311.16101">Unicorns</a> were accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>! VFusion3D is the first work exploring scalable 3D generative/reconstruction models as a step towards a 3D foundation. Check it out if you are interested in 3D generation.</li>
  <li class="news-item-hidden">[04.2024] We are organizing <a href="https://icml-fm-wild.github.io/">ICML 2024 Foundation Models in the Wild Workshop</a>, submissions on related topics are very welcomed!</li>
  <li class="news-item-hidden">[10.2023] Starting my PhD at Meta and Oxford!</li>
  <li class="news-item-hidden">[07.2023] Hyperbolic Audio-visual Zero-shot Learning was accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>!</li>
  <li class="news-item-hidden">[07.2023] Graduated from ANU with First-Class Honours.</li>
  <li class="news-item-hidden">[10.2022] Awarded as a top reviewer in <a href="https://nips.cc/Conferences/2022/ProgramCommittee">NeurIPS 2022</a>!</li>
  <li class="news-item-hidden">[07.2022] Blind Image Decomposition (BID) was accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>! A new low-level vision task that better adapts to complex real-world scenarios. Check our <a href="https://junlinhan.github.io/projects/BID.html">project page</a> for more.</li>
  <li class="news-item-hidden">[05.2022] You Only Cut Once (YOCO) was accepted to <a href="https://icml.cc/">ICML 2022</a>! Check <a href="https://arxiv.org/abs/2201.12078">here</a> for our work on how to perform data augmentation.</li>
  <li class="news-item-hidden">[12.2021] Joined <a href="https://www.adelaide.edu.au/aiml/">Australian Institute for Machine Learning (AIML)</a>, University of Adelaide as a visiting research intern, hosted by <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>!</li>
</ul>
<span id="expand-news" style="cursor: pointer; color: black; text-decoration: underline;">show more</span>

<style>
  .news-item-hidden {
    display: none;
  }
</style>

<script>
  const expandNews = document.getElementById('expand-news');
  const hiddenNewsItems = document.querySelectorAll('.news-item-hidden');
  let expanded = false;

  expandNews.addEventListener('click', () => {
    hiddenNewsItems.forEach(item => {
      item.style.display = expanded ? 'none' : 'list-item';
    });
    expanded = !expanded;
    expandNews.textContent = expanded ? 'show less' : 'show more';
  });
</script>

    

    
<h2 id="Interest">Research Interests</h2>
<p>
My research focuses on multimodal foundation models, including multimodal large language models and world generation models, as well as their interactions and integration. You can click on to expand some projects:
</p>

<!-- Removed the 'You can click on to expand:' text and all <br> tags -->

<details>
  <summary> (1) Understanding the world through multimodal foundation models</summary>
  <div style="padding-top: 5px; padding-left: 15px;">
  Selected projects: <a href="https://junlinhan.github.io/projects/lsbs">Visual priors in LLMs</a>, <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Llama4</a>
  </div>
</details>
<br>
<details>
  <summary>(2) Generation (usually 3D) of the world</summary>
  <div style="padding-top: 5px; padding-left: 15px;">
  Selected projects: <a href="https://junlinhan.github.io/projects/vfusion3d.html">Scaling-up 3D gen</a>, <a href="https://junlinhan.github.io/projects/flex3d">High-quality 3D asset gen</a>
  </div>
</details>
<br>
<details>
  <summary>(3) Interactions</summary>
  <div style="padding-top: 5px; padding-left: 15px;">
  Selected projects:  <a href="https://chuny1.github.io/3DGPT/3dgpt.html">LLM for 3D world modeling</a>
  </div>
</details>

  




</ul>




<A NAME="Education"><h2>Education</h2></A>
<b>Postgraduate (01.2024 - ongoing)</b>
<ul>
<li>PhD in Computer Vision, <a href="https://www.ox.ac.uk/"> University of Oxford </a>
</li>
&nbsp; Supervised by <a href="https://en.wikipedia.org/wiki/Philip_Torr"> Prof. Philip Torr</a>
</ul>

<b>Undergraduate (02.2019 - 07.2023)</b>
<ul>
<li>B.S. Information Technology (Honours), <a href="https://www.anu.edu.au/">ANU</a>
</li>
&nbsp; Supervised by  <a href="https://people.csiro.au/P/L/Lars-Petersson">Dr. Lars Petersson</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>, and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>.
</ul>



<A NAME="Experience"><h2>Research Experience</h2></A>
<b>PhD Student Researcher at GenAI, Meta (10.2023 - ongoing) </b>
<ul>
<li> Topics: LLM, MLLM, 3D&4D generation</li>
<li> Hosted by <a href="https://fkokkinos.github.io/">Dr. Filippos Kokkinos</a>
</ul>

<b>Lead Research Scientist, Cybever (05.2023 - 09.2023) </b>
<ul>
<li> Topics:  Generative models for 3D world modeling </li>
</ul>

<b>Visiting Research Intern at AIML, University of Adelaide (12.2021 - 05.2023)</b>
<ul>
<li> Topics: Data-centric AI, Memorability </li>
<li> Advised by <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>
</ul>

<b>Research Student at Data61-CSIRO & ANU (08.2020 - 05.2023)</b>
<ul>
<li> Topics: Generative models, Self-supervised learning </li>
<li> Advised by <a href="https://people.csiro.au/P/L/Lars-Petersson">Dr. Lars Petersson</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>
</ul>



<script type="text/javascript">
  const publicationsData = [
      {
      title: 'Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training',
      img: './paperimages/lsbs.png',
      author: '<b>Junlin Han</b>, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos',
      note: 'Preprint, 2025',
      link: [
        ["https://junlinhan.github.io/projects/lsbs","Project page"], ["https://arxiv.org/abs/2509.26625","arXiv"], ["https://arxiv.org/pdf/2509.26625.pdf","pdf"],  ["./bibtex/lsbs.txt","BibTex"]
      ],
      highlight: true
    },
    {
      title: 'Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model and Input View Curation',
      img: './projects/flex3d/videos/teaser/splash_video.mp4',
      author: '<b>Junlin Han</b>, Jianyuan Wang, Andrea Vedaldi, Philip Torr, Filippos Kokkinos',
      note: 'ICML, 2025',
      link: [
        ["https://junlinhan.github.io/projects/flex3d","Project page"], ["https://arxiv.org/abs/2410.00890","arXiv"], ["https://arxiv.org/pdf/2410.00890.pdf","pdf"], ["./Files/flex3d_poster.pdf","Poster"], ["./bibtex/paper19.txt","BibTex"]
      ],
      highlight: true
    },
    {
      title: 'VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models',
      img: './paperimages/paper16.png',
      author: '<b>Junlin Han*</b>, Filippos Kokkinos*, Philip Torr',
      note: 'ECCV, 2024',
      link: [
        ["https://junlinhan.github.io/projects/vfusion3d.html","Project page"], ["https://arxiv.org/abs/2403.12034","arXiv"], ["https://arxiv.org/pdf/2403.12034.pdf","pdf"], ["https://github.com/facebookresearch/vfusion3d","Code"], ["https://huggingface.co/spaces/facebook/VFusion3D","Demo"], ["./Files/vfusion3d_poster.pdf","Poster"], ["./bibtex/paper16.txt","BibTex"]
      ],
      highlight: true
    },
    {
      title: 'What Images are More Memorable to Machines?',
      img: './paperimages/paper9.png',
      author: '<b>Junlin Han</b>, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, Ian Reid',
      note: 'Preprint, 2023',
      link: [
        ["https://junlinhan.github.io/projects/machinemem.html","Project page"], ["https://arxiv.org/abs/2211.07625","arXiv"], ["https://arxiv.org/pdf/2211.07625.pdf","pdf"], ["https://github.com/JunlinHan/MachineMem","Code"], ["./bibtex/paper9.txt","BibTex"]
      ],
      highlight: true
    },
    {
      title: 'You Only Cut Once: Boosting Data Augmentation with a Single Cut',
      img: './paperimages/paper7.jpg',
      author: '<b>Junlin Han</b>, Pengfei Fang, Weihao Li, Jie Hong, Ali Armin, Ian Reid, Lars Petersson, Hongdong Li',
      note: 'ICML, 2022',
      link: [
        ["https://arxiv.org/abs/2201.12078","arXiv"], ["https://arxiv.org/pdf/2201.12078.pdf","pdf"], ["https://proceedings.mlr.press/v162/han22a.html","PMLR"], ["https://github.com/JunlinHan/YOCO","Code"], ["./Files/YOCO_slide.pptx","Slide"], ["./bibtex/paper7.txt","BibTex"]
      ],
      highlight: true
    },
    {
      title: 'Blind Image Decomposition',
      img: './projects/resources/paper5/BID.gif',
      author: '<b>Junlin Han</b>, Weihao Li, Pengfei Fang, Chunyi Sun, Jie Hong, Ali Armin, Lars Petersson, Hongdong Li',
      note: 'ECCV, 2022',
      link: [
        ["https://junlinhan.github.io/projects/BID.html","Project page"], ["https://arxiv.org/abs/2108.11364","arXiv"], ["https://arxiv.org/pdf/2108.11364.pdf","pdf"], ["https://github.com/JunlinHan/BID","Code"], ["https://github.com/JunlinHan/BID","Dataset"], ["https://youtu.be/wkyJDjUPCkg","Video"], ["./Files/BID_slide.pptx","Slide"], ["./Files/BID_poster.pdf","Poster"], ["./bibtex/paper5.txt","BibTex"]
      ],
      highlight: true
    },
      {
      title: 'Scaling Sequence-to-Sequence Generative Neural Rendering',
      img: './paperimages/monet.gif',
      author: 'Shikun Liu, Kam Woh Ng, Wonbong Jang, Jiadong Guo, <b>Junlin Han</b>, Haozhe Liu, Yiannis Douratsos, Juan C. Pérez, Zijian Zhou, Chi Phung, Tao Xiang, Juan-Manuel Pérez-Rúa',
      note: 'Preprint, 2025',
      link: [
        ["https://shikun.io/projects/kaleido","Project page"], ["https://arxiv.org/abs/2510.04236","arXiv"], ["https://arxiv.org/pdf/2510.04236.pdf","pdf"],  ["./bibtex/kaleido.txt","BibTex"]
      ],
      highlight: false
    },    
    {
      title: 'Unsupervised Decomposition of 3D Shapes into Expressive and Editable Extruded Profile Primitives',
      img: './paperimages/3D2EP.png',
      author: 'Chunyi Sun, <b>Junlin Han</b>, Runjia Li, Weijian Deng, Dylan Campbell, Stephen Gould',
      note: 'SIGGRAPH, 2025',
      link: [
         ["https://dl.acm.org/doi/full/10.1145/3721238.3730704","paper"], ["https://dl.acm.org/doi/pdf/10.1145/3721238.3730704","pdf"], ["https://scholar.googleusercontent.com/scholar.bib?q=info:IULraMmtx7MJ:scholar.google.com/&output=citation&scisdr=CgJAYgDxEPn1jjtqnV4:AAZF9b8AAAAAaIZshV7Shz58q2zkq69xBOL87SI&scisig=AAZF9b8AAAAAaIZshWdOKMOS0VYy3Qb_ab5nJ-c&scisf=4&ct=citation&cd=-1&hl=en&authuser=1","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning',
      img: './paperimages/HallucinationataGlance.png',
      author: 'Tianyi Bai, Yuxuan Fan, Jiantao Qiu, Fupeng Sun, Jiayi Song, <b>Junlin Han</b>, Zichen Liu, Conghui He, Wentao Zhang, Binhang Yuan',
      note: 'NeurIPS, 2025',
      link: [
        ["https://github.com/beccabai/hallu_med","Dataset"], ["https://arxiv.org/abs/2506.07227","arXiv"], ["https://arxiv.org/pdf/2506.07227.pdf","pdf"], ["https://scholar.googleusercontent.com/scholar.bib?q=info:iwLCsytqgl0J:scholar.google.com/&output=citation&scisdr=CgLmhzyBEPznyIxSUo8:AAZF9b8AAAAAaE5USo_7LFDC_KItrAY0lZJPkgs&scisig=AAZF9b8AAAAAaE5USqqDkHz5W55ct5UxyTg7X2k&scisf=4&ct=citation&cd=-1&hl=en","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'AuralSAM2: Enabling SAM2 Hear Through Pyramid Audio-Visual Feature Prompting',
      img: './paperimages/AuralSAM2.png',
      author: 'Yuyuan Liu, Yuanhong Chen, Chong Wang, <b>Junlin Han</b>, Junde Wu, Can Peng, Jingkun Chen, Yu Tian, Gustavo Carneiro',
      note: 'Preprint, 2025',
      link: [
        ["https://arxiv.org/abs/2506.01015","arXiv"], ["https://arxiv.org/pdf/2506.01015.pdf","pdf"], ["https://scholar.googleusercontent.com/scholar.bib?q=info:HHm61fc80eAJ:scholar.google.com/&output=citation&scisdr=CgLmhzyBEPznyIxSdwU:AAZF9b8AAAAAaE5UbwWeUZSrXNvmiyt429WaXCM&scisig=AAZF9b8AAAAAaE5Ub7VtAFf3yfSfXEg1Mq7dqns&scisf=4&ct=citation&cd=-1&hl=en","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation',
      img: './paperimages/llama4.png',
      author: 'Llama4 team',
      note: 'Models, 2025',
      link: [
        ["https://ai.meta.com/blog/llama-4-multimodal-intelligence/","project page"], ["https://huggingface.co/collections/meta-llama/llama-4-67f0c30d9fe03840bc9d0164","Models"]
      ],
      highlight: false
    },
    {
      title: 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models',
      img: './paperimages/vgrp.png',
      author: 'Yufan Ren, Konstantinos Tertikas, Shalini Maiti, <b>Junlin Han</b>, Tong Zhang, Sabine Süsstrunk, Filippos Kokkinos',
      note: 'Preprint, 2025',
      link: [
        ["https://yufan-ren.com/subpage/VGRP-Bench/index.html","Project page"], ["https://arxiv.org/abs/2503.23064","arXiv"], ["https://arxiv.org/pdf/2503.23064","pdf"], ["https://github.com/ryf1123/VGRP-Bench","Code"], ["https://huggingface.co/datasets/VGRP-Bench/VGRP-Bench","Dataset"], ["./bibtex/vgrp.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model',
      img: './paperimages/paper21.png',
      author: 'Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, <b>Junlin Han</b>, Ender Konukoglu, Serge Belongie',
      note: 'CVPR, 2025',
      link: [
        ["https://arxiv.org/abs/2503.16282","arXiv"], ["https://arxiv.org/pdf/2503.16282","pdf"], ["https://github.com/ZhaochongAn/GFS-VL","Code"], ["./bibtex/paper21.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: '3D-GPT: Procedural 3D Modeling with Large Language Models',
      img: './paperimages/paper13.png',
      author: 'Chunyi Sun*, <b>Junlin Han*</b>, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould',
      note: '3DV, 2025',
      link: [
        ["https://chuny1.github.io/3DGPT/3dgpt.html","Project page"], ["https://arxiv.org/abs/2310.12945","arXiv"], ["https://arxiv.org/pdf/2310.12945.pdf","pdf"], ["https://github.com/Chuny1/3DGPT","Code"], ["./bibtex/paper13.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer',
      img: './paperimages/paper18.jpg',
      author: 'Runjia Li, <b>Junlin Han</b>, Luke Melas-Kyriazi, Chunyi Sun, Zhaochong An, Zhongrui Gui, Shuyang Sun, Philip Torr, Tomas Jakab',
      note: '3DV, 2025',
      link: [
        ["https://dreambeast3d.github.io/","Project page"], ["https://arxiv.org/abs/2409.08271","arXiv"], ["https://arxiv.org/pdf/2409.08271.pdf","pdf"], ["https://github.com/runjiali-rl/threestudio-dreambeast","Code"], ["./bibtex/paper18.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Semantic Score Distillation Sampling for Compositional Text-to-3D Generation',
      img: './paperimages/paper20.jpg',
      author: 'Ling Yang, Zixiang Zhang, <b>Junlin Han</b>, Bohan Zeng, Runjia Li, Philip Torr, Wentao Zhang',
      note: 'Preprint, 2024',
      link: [
        ["https://arxiv.org/abs/2410.09009","arXiv"], ["https://arxiv.org/pdf/2410.09009.pdf","pdf"], ["./bibtex/paper20.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Learning-based Multi-View Stereo: A Survey',
      img: './paperimages/paper17.jpg',
      author: 'Fangjinhua Wang*, Qingtian Zhu*, Di Chang*, Quankai Gao, <b>Junlin Han</b>, Tong Zhang, Richard Hartley, Marc Pollefeys',
      note: 'Preprint, 2024',
      link: [
        ["https://arxiv.org/abs/2408.15235","arXiv"], ["https://arxiv.org/pdf/2408.15235.pdf","pdf"], ["./bibtex/paper17.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Strong and Controllable Blind Image Decomposition',
      img: './paperimages/paper15.png',
      author: 'Zeyu Zhang*, <b>Junlin Han*</b>, Chenhui Gou*, Hongdong Li, Liang Zheng',
      note: 'Preprint, 2024',
      link: [
        ["https://arxiv.org/abs/2403.10520","arXiv"], ["https://arxiv.org/pdf/2403.10520.pdf","pdf"], ["./bibtex/paper15.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs',
      img: './paperimages/paper14.png',
      author: 'Haoqin Tu*, Chenhang Cui*, Zijun Wang*, Yiyang Zhou, Bingchen Zhao, <b>Junlin Han</b>, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie',
      note: 'ECCV, 2024',
      link: [
        ["https://arxiv.org/abs/2311.16101","arXiv"], ["https://arxiv.org/pdf/2311.16101.pdf","pdf"], ["https://github.com/UCSC-VLAA/vllm-safety-benchmark","Code"], ["./bibtex/paper14.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'NeRFEditor: Differentiable Style Decomposition for 3D Scene Editing',
      img: './paperimages/paper10.png',
      author: 'Chunyi Sun, Yanbin Liu, <b>Junlin Han</b>, Stephen Gould',
      note: 'WACV, 2024',
      link: [
        ["https://chuny1.github.io/NeRFEditor/nerfeditor.html","Project page"], ["https://arxiv.org/abs/2212.03848","arXiv"], ["https://arxiv.org/pdf/2212.03848.pdf","pdf"], ["./bibtex/paper10.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Hyperbolic Audio-visual Zero-shot Learning',
      img: './paperimages/paper11.jpg',
      author: 'Jie Hong, Zeeshan Hayder, <b>Junlin Han</b>, Pengfei Fang, Mehrtash Harandi, Lars Petersson',
      note: 'ICCV, 2023',
      link: [
        ["https://arxiv.org/abs/2308.12558","arXiv"], ["https://arxiv.org/pdf/2308.12558.pdf","pdf"], ["https://github.com/JHome1/Hyper-AVZSL","Code"], ["./bibtex/paper11.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Curved Geometric Networks for Visual Anomaly Recognition',
      img: './paperimages/paper6.JPG',
      author: 'Jie Hong, Pengfei Fang, Weihao Li, <b>Junlin Han</b>, Lars Petersson, Mehrtash Harandi',
      note: 'TNNLS, 2023',
      link: [
        ["https://ieeexplore.ieee.org/abstract/document/10244211","Paper"], ["https://arxiv.org/abs/2208.01188","arXiv"], ["https://arxiv.org/pdf/2208.01188.pdf","pdf"], ["https://github.com/JHome1/GiO-GiT","Code"], ["./bibtex/paper6.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'GOSS: Towards Generalized Open-set Semantic Segmentation',
      img: './paperimages/paper3.png',
      author: 'Jie Hong, Weihao Li, <b>Junlin Han</b>, Jiyang Zheng, Pengfei Fang, Mehrtash Harandi, Lars Petersson',
      note: 'The Visual Computer, 2023',
      link: [
        ["https://link.springer.com/article/10.1007/s00371-023-02925-8","Paper"], ["https://arxiv.org/abs/2203.12116","arXiv"], ["https://arxiv.org/pdf/2203.12116.pdf","pdf"], ["https://github.com/JHome1/GOSS_Segmentor","Code"], ["./bibtex/paper3.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'CropMix: Sampling a Rich Input Distribution via Multi-Scale Cropping',
      img: './paperimages/paper8.png',
      author: '<b>Junlin Han</b>, Lars Petersson, Hongdong Li, Ian Reid',
      note: 'Preprint, 2022',
      link: [
        ["https://arxiv.org/abs/2205.15955","arXiv"], ["https://arxiv.org/pdf/2205.15955.pdf","pdf"], ["https://github.com/JunlinHan/cropmix","Code"], ["./bibtex/paper8.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Underwater Image Restoration via Contrastive Learning and a Real-world Dataset',
      img: './paperimages/paper4.JPG',
      author: '<b>Junlin Han</b>, Mehrdad Shoeiby, Tim Malthus, Elizabeth Botha, Janet Anstee, Saeed Anwar, Ran Wei, Ali Armin, Hongdong Li, Lars Petersson',
      note: 'Remote Sensing (best paper award), 2022',
      link: [
        ["https://www.mdpi.com/2072-4292/14/17/4297","Paper"], ["https://github.com/JunlinHan/CWR","Code"], ["https://github.com/JunlinHan/CWR","Dataset"], ["./bibtex/paper4.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Single Underwater Image Restoration by Contrastive Learning',
      img: './paperimages/paper2.JPG',
      author: '<b>Junlin Han</b>, Mehrdad Shoeiby, Tim Malthus, Elizabeth Botha, Janet Anstee, Saeed Anwar, Ran Wei, Lars Petersson, Ali Armin',
      note: 'IGARSS (oral), 2021',
      link: [
        ["https://arxiv.org/abs/2103.09697","arXiv"], ["https://arxiv.org/pdf/2103.09697.pdf","pdf"], ["https://github.com/JunlinHan/CWR","Code"], ["https://github.com/JunlinHan/CWR","Dataset"], ["./bibtex/paper2.txt","BibTex"]
      ],
      highlight: false
    },
    {
      title: 'Dual Contrastive Learning for Unsupervised Image-to-Image Translation',
      img: './paperimages/paper1.PNG',
      author: '<b>Junlin Han</b>, Mehrdad Shoeiby, Lars Petersson, Ali Armin',
      note: 'NTIRE, CVPRW (oral), 2021',
      link: [
        ["https://arxiv.org/abs/2104.07689","arXiv"], ["https://arxiv.org/pdf/2104.07689.pdf","pdf"], ["https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Han_Dual_Contrastive_Learning_for_Unsupervised_Image-to-Image_Translation_CVPRW_2021_paper.pdf","CVF"], ["https://github.com/JunlinHan/DCLGAN","Code"], ["https://youtu.be/w0oltXvLgmI","Video"], ["./Files/DCLGAN_slide.pptx","Slide"], ["./bibtex/paper1.txt","BibTex"]
      ],
      highlight: false
    }
    // To add a new paper, just copy one of the blocks above and paste it here.
  ];

  // Sort publications by note (year) in descending order.
  // This function tries to extract a 4-digit year from the 'note' string.
  publicationsData.sort((a, b) => {
    const yearA = (a.note.match(/\d{4}/) || [0])[0];
    const yearB = (b.note.match(/\d{4}/) || [0])[0];
    return yearB - yearA;
  });

  // ==================================================
  // 2. RENDERING FUNCTION: DO NOT EDIT THIS
  // ==================================================
  // This function takes a publication object and creates the HTML.
  function add_publication(pub) {
    // Determine the title style based on the highlight flag
    const displayTitle = pub.highlight 
      ? `<font color="green">${pub.title}</font>` 
      : pub.title;

    document.write('<tr>');
    
    // Media (Image/Video) column
    const mediaUrl = pub.link && pub.link.length > 0 ? pub.link[0][0] : null;
    const mediaTag = pub.img.endsWith('.mp4')
      ? `<video width="230" height="130" autoplay loop muted playsinline><source src="${pub.img}" type="video/mp4">Your browser does not support the video tag.</video>`
      : `<img width="230" height="130" src="${pub.img}" alt="Teaser for ${pub.title}">`;
    
    document.write('<td><div style="width: 230px">');
    if (mediaUrl) {
      document.write(`<a target="_blank" href="${mediaUrl}">${mediaTag}</a>`);
    } else {
      document.write(mediaTag);
    }
    document.write('</div></td>');

    // Details column
    document.write('<td>');
    document.write(`<p class="title">${displayTitle}</p>`);
    document.write(`<p class="author">${pub.author}</p>`);
    if (pub.note) {
      document.write(`<p class="note">${pub.note}</p>`);
    }
    if (pub.link && pub.link.length > 0) {
      document.write('<p class="link">');
      pub.link.forEach(linkItem => {
        document.write(`<a target="_blank" href="${linkItem[0]}">[${linkItem[1]}]</a> `);
      });
      document.write('</p>');
    }
    document.write('</td></tr>');
  }
</script>


<!-- HTML STRUCTURE AND BUTTONS (Unchanged) -->
<A NAME="Publications">
  <h2>Publications</h2>
</A>
<p>Representative papers (as leading author) are <font color="green">highlighted</font>.</p>

<style>
  .pub-buttons { margin-bottom: 1em; }
  .pub-buttons button {
    padding: 10px 20px;
    background-color: #4c84af;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s;
    margin-right: 0.5em;
  }
  .pub-content { display: none; }
  .pub-content.active { display: block; }
  /* Optional: Add some basic styling for the table */
  .pub-content table {
    width: 100%;
    border-collapse: separate;
  }
  .pub-content td {
    vertical-align: middle;
    padding-right: 0.5em;
    padding-bottom: 0.1em; /* 新增此行来控制垂直间距 */
  }
  .pub-content p {
    margin: 0.2em 0;
  }
</style>

<div class="pub-buttons">
  <button data-target="selected">Show Selected Papers</button>
  <button data-target="all">Show All Papers</button>
</div>

<!-- ==================================================
// 3. DYNAMIC RENDERING SECTIONS
// ================================================== -->

<!-- Selected Papers -->
<div id="selected" class="pub-content active">
  <table>
    <script>
      // Loop through the data and render ONLY highlighted papers
      publicationsData.forEach(pub => {
        if (pub.highlight) {
          add_publication(pub);
        }
      });
    </script>
  </table>
</div>


<!-- All Papers -->
<div id="all" class="pub-content">
  <table>
    <script>
      // Loop through the data and render ALL papers
      publicationsData.forEach(pub => {
        add_publication(pub);
      });
    </script>
  </table>
</div>

<!-- Button Click Handler (Unchanged) -->
<script>
  document.querySelectorAll('.pub-buttons button').forEach(btn => {
    btn.addEventListener('click', () => {
      const target = btn.getAttribute('data-target');
      document.querySelectorAll('.pub-content').forEach(div => {
        div.classList.toggle('active', div.id === target);
      });
    });
  });
</script>


<A NAME="Services"><h2>Academic Services</h2></A>
<ul>
<li><b>Workshop Organization</b></li>
Foundation Models in the Wild at ICML 2024 and ICLR 2025
<br>
<li><b>Conference review</b></li>
CVPR 2022 2023 2024 2025, ICCV 2023, ECCV 2022 2024, ICML 2022 2024 2025, NeurIPS 2022 2023, ICLR 2023 2024 2025 2026, COLM 2025, SIGGRAPH Asia 2024 2025, ACCV 2022, BMVC 2023, WACV 2024, AAAI 2023
<br>
<li><b>Journal review</b> </li>
Transactions on Pattern Analysis and Machine Intelligence (TPAMI),  International Journal of Computer Vision (IJCV), Transactions on Image Processing (TIP), Transactions on Geoscience and Remote Sensing (TGRS)
</ul>
</font>

<!-- <A NAME="Skills"><h2>Skills</h2></A>
<ul>
<li><b>Programming Language</b></li>
Proficient in MATLAB, Python
<br>
Familiar with Java, Haskell, R, CSS, Html, Assembly (ARMV7), SQL
<Li><b>Tool</b></li>
NumPy, PyTorch, Git, LaTeX
<li><b>Language</b></li>
English, Singlish, and Madarian
</ul>
</font> -->

<A NAME="Awards"><h2>Awards</h2></A>
<ul>
  <li><b>Meta PhD Scholarship, 2023</b></br>
    for PhD research at University of Oxford
    </li>
    <li><b>Best paper award, 2022</b></br>
      from Remote Sensing journal
      </li>
    <li><b>Chancellor's Letter of Commendation, 2022</b></br>
      for best academic performance at Australian National University
  </li>
  <li><b>Top Reviewer, 2022</b></br>
    NeurIPS (Conference on Neural Information Processing Systems)
</li>
<li><b>Second Best Presentation Award, 2021</b></br>
  AIM (Active Integrated Matter) Conference
  </li>
<li><b>Top-up Scholarship, 2021</b></br>
for research work at Data61-CSIRO
</li>
<li><b>Undergraduate Vacation Scholarship, 2020</b></br>
for summer research at Data61-CSIRO
</li>
</ul>
</font>

<A NAME="Miscs"><h2>Miscs</h2></A>
<ul>
<li>At the beginning, I was born and raised in <a href="https://en.wikipedia.org/wiki/Chengdu">Chengdu</a>, which is famous for spice and pandas. </li>
<li>I studied and lived in China, Singapore, Australia, and UK.</li>
<li>Aside from research, I am passionate about traveling (especially nature travel), cooking, gaming, and swimming.   </li>
</ul>
</font>

</br>
</br>

<table class="sub-table" style="width: 200px;height: 100px;" align="center">
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=A_M689vS_rmlsTbtLHxcdi_-Ti8JbhM0uQ_c6itmJb8&cl=ffffff&w=a"></script>
  </table>

</body>
</html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="Files/jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Junlin Han</title>
</head>
<body>

<a id="home" class="anchor"></a>
<div id="container">
<div class="container">
<table class="imgtable"><tr><td>
<a><img src="./Files/face.jpg" alt="" align="left" height="200px"/></a></td>
<td align="left"><p><font size="4"> <b> Junlin (Hans) Han </b> </font><font size="4"; font style="font-family:Microsoft YaHei"></font><font size="4"></font><br />
<!-- Prounce
<br> -->
 <!--
<br
<!-- <i> Undergraduate student at ANU</i>
<br>
<i> Research student, Data61-CSIRO &】 ANU</i>
<br>
<i> Visiting research intern at AIML, University of Adelaide</i>
<br /><br /> -->
<a href="https://ai.meta.com/genai/">GenAI</a>, <a href="https://about.meta.com/"> Meta </a></br >
<a href="https://torrvision.com/index.html"> Torr Vision Group </a>, <a href="https://www.ox.ac.uk/"> University of Oxford </a></br>
<br />Location: Meta Kings Cross, London N1C 4DB  <br>
<class="staffshortcut">
 <A HREF="#Profile">Profile</A> |
<A HREF="#News">News</A> |
 <A HREF="#Interest">Research Interests</A> |
 <A HREF="#Education">Education</A> |
  <A HREF="#Education">Experience </A> |
 <A HREF="#Publications">Publications</A> |
 <A HREF="#Services">Services</A> |
 <A HREF="#Skills">Skills</A> |
 <A HREF="#Awards">Awards</A>|
  <A HREF="#Miscs">Miscs</A>
<br />
Email: junlinhcv@gmail.com; junlinhan@meta.com; junlin.han@eng.ox.ac.uk<br />
[<a href="https://github.com/junlinhan" target="_blank">GitHub</a>]
[<a href="./Files/CV.pdf">Curriculum Vitae</a>]
[<a href="https://scholar.google.com/citations?user=5L0Uj_IAAAAJ&hl=en&authuser=1&oi=ao" target="_blank">Google Scholar</a>]
[<a href="https://twitter.com/han_junlin" target="_blank">Twitter/X</a>]
</td></tr></table>





<A NAME="Profile"><h2>Profile</h2></A>
<b>Greetings, welcome to my website!</b>
<ul>
  <li>I am a researcher at <a href="https://ai.meta.com/genai/">GenAI</a>, <a href="https://about.meta.com/">Meta</a>, and a second-year PhD (DPhil) student at the <a href="https://torrvision.com/index.html">Torr Vision Group</a>, <a href="https://www.ox.ac.uk/">University of Oxford</a>, working with <a href="https://en.wikipedia.org/wiki/Philip_Torr">Prof. Philip Torr</a>. 
    Before that, I worked as a research scientist at <a href="https://www.cybever.ai/">Cybever</a>, a 3D AIGC startup for a few months. 
    I obtained my BSc from the <a href="https://www.anu.edu.au/">Australian National University</a> under the supervision of <a href="https://people.csiro.au/P/L/Lars-Petersson">Dr. Lars Petersson</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>, and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>.</li>
<li>I acknowledge that information asymmetry can significantly hinder research opportunities for junior students. If you’re interested in chatting about life, research, or potential collaborations, feel free to email me. </li>
</ul>
<!-- <li> For undergrad and master students at Oxford only: If you are interested in an internship at TVG working on 3D vision or MLLM with me, please email me your current CV to discuss further. </li>
-->


      
<A NAME="News"><h2>News</h2></A>
<ul id="news-list">
  <li>[03.2025] One paper <a href="https://arxiv.org/abs/2503.16282">on point-cloud segmentation</a> was accepted to CVPR 2025, congrats to <a href="https://zhaochongan.github.io/">Zhaochong</a>!</li>
  <li>[12.2024] Our previous <a href="https://www.mdpi.com/2072-4292/14/17/4297">work on low-level vision</a> won the best paper award in Remote Sensing!</li>
  <li>[12.2024] We will be organizing the <a href="https://fm-wild-community.github.io/">2nd Workshop on Foundation Models in the Wild</a> at ICLR 2025!</li> 
  <li>[11.2024] Two papers (<a href="https://chuny1.github.io/3DGPT/3dgpt.html">3D-GPT</a> and <a href="https://dreambeast3d.github.io/">DreamBeast</a>) were accepted to <a href="https://3dvconf.github.io/2025/">3DV 2025</a>. Congratulations to <a href="https://chuny1.github.io/">Chunyi</a> and <a href="https://runjiali-rl.github.io/">Runjia</a>! 3D-GPT is pioneering work in large-scale 3D scene generation, and DreamBeast is a fascinating project generating fantastical 3D animals.</li>
  <li>[07.2024] <a href="https://junlinhan.github.io/projects/vfusion3d.html">VFusion3D</a> and <a href="https://arxiv.org/abs/2311.16101">Unicorns</a> were accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>! VFusion3D is the first work exploring scalable 3D generative/reconstruction models as a step towards a 3D foundation. Check it out if you are interested in 3D generation.</li>
  <li class="news-item-hidden">[04.2024] We are organizing <a href="https://icml-fm-wild.github.io/">ICML 2024 Foundation Models in the Wild Workshop</a>, submissions on related topics are very welcomed!</li>
  <li class="news-item-hidden">[10.2023] Starting my PhD at Meta and Oxford!</li>
  <li class="news-item-hidden">[07.2023] Hyperbolic Audio-visual Zero-shot Learning was accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>!</li>
  <li class="news-item-hidden">[07.2023] Graduated from ANU with First-Class Honours.</li>
  <li class="news-item-hidden">[10.2022] Awarded as a top reviewer in <a href="https://nips.cc/Conferences/2022/ProgramCommittee">NeurIPS 2022</a>!</li>
  <li class="news-item-hidden">[07.2022] Blind Image Decomposition (BID) was accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>! A new low-level vision task that better adapts to complex real-world scenarios. Check our <a href="https://junlinhan.github.io/projects/BID.html">project page</a> for more.</li>
  <li class="news-item-hidden">[05.2022] You Only Cut Once (YOCO) was accepted to <a href="https://icml.cc/">ICML 2022</a>! Check <a href="https://arxiv.org/abs/2201.12078">here</a> for our work on how to perform data augmentation.</li>
  <li class="news-item-hidden">[12.2021] Joined <a href="https://www.adelaide.edu.au/aiml/">Australian Institute for Machine Learning (AIML)</a>, University of Adelaide as a visiting research intern, hosted by <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>!</li>
</ul>
<span id="expand-news" style="cursor: pointer; color: black; text-decoration: underline;">show more</span>

<style>
  .news-item-hidden {
    display: none;
  }
</style>

<script>
  const expandNews = document.getElementById('expand-news');
  const hiddenNewsItems = document.querySelectorAll('.news-item-hidden');
  let expanded = false;

  expandNews.addEventListener('click', () => {
    hiddenNewsItems.forEach(item => {
      item.style.display = expanded ? 'none' : 'list-item';
    });
    expanded = !expanded;
    expandNews.textContent = expanded ? 'show less' : 'show more';
  });
</script>

    

<A NAME="Interest"><h2>Research Interests</h2></A>
<ul>
<li>Recognizing that data is the <b>core source</b> from which AI's capabilities emerge, I study how data can be effectively created, utilized, and understood to enhance the capabilities of AI systems.
  
  <li>   My work spans the <b>entire data lifecycle</b>, from the initial creation and manipulation of visual data to the development of methodologies that strategically leverage diverse data sources for improved model training and performance. Moreover, I investigate the fundamental principles that govern the interaction between data and AI, striving to understand how data shapes intelligence and decision-making. Through this multifaceted exploration of data, my research aims to pave the way for more powerful, efficient, reliable, and ultimately, more human-aligned AI. </li>
You can click on to expand detailed projects:

<details>
  <summary>(1) <b>Generation</b> and <b>manipulation</b> of 2D, 3D, and 4D visual data,</summary>
  <br>
  Selected projects:  <a href="https://junlinhan.github.io/projects/BID.html">Image decomposition</a>,  <a href="https://chuny1.github.io/3DGPT/3dgpt.html">LLM for 3D scene generation</a>, <a href="https://junlinhan.github.io/projects/flex3d">High-quality 3D asset generation</a>
  <br>
</details>
<br>
<details>
  <summary>(2) Use <b>augmented, synthetic, and distilled data</b> for training AI models,</summary>
  <br>
 Selected projects: <a href="https://github.com/JunlinHan/YOCO">How to perform data augmentation</a>, <a href="https://junlinhan.github.io/projects/vfusion3d.html">Synthetic data for training 3D generative models</a>
  <br>
</details>
<br>
<details>
  <summary>(3) Understand the <b>role of data</b> in AI. </summary>
  <br>
  Selected projects: <a href="https://junlinhan.github.io/projects/machinemem.html">Memorability of images</a>
  <br>
</details>
<br>
I am also interested in extending my research to other scientific domains, such as exploring the applications of AI in the field of health and biology.



</ul>




<A NAME="Education"><h2>Education</h2></A>
<b>Postgraduate (01.2024 - ongoing)</b>
<ul>
<li>PhD in Computer Vision, <a href="https://www.ox.ac.uk/"> University of Oxford </a>
</li>
&nbsp; Supervised by <a href="https://en.wikipedia.org/wiki/Philip_Torr"> Prof. Philip Torr</a>
</ul>

<b>Undergraduate (02.2019 - 07.2023)</b>
<ul>
<li>B.S. Information Technology (Honours), <a href="https://www.anu.edu.au/">ANU</a>
</li>
&nbsp; Supervised by  <a href="https://people.csiro.au/P/L/Lars-Petersson">Dr. Lars Petersson</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>, and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>.
</ul>



<A NAME="Experience"><h2>Research Experience</h2></A>
<b>PhD Student Researcher at GenAI, Meta (10.2023 - ongoing) </b>
<ul>
<li> Topics: 3D&4D generation, Multimodal Large Language Models</li>
<li> Hosted by <a href="https://fkokkinos.github.io/">Dr. Filippos Kokkinos</a>
</ul>

<b>Research Scientist, Cybever (05.2023 - 09.2023) </b>
<ul>
<li> Topics:  3D generation </li>
</ul>

<b>Visiting Research Intern at AIML, University of Adelaide (12.2021 - 05.2023)</b>
<ul>
<li> Topics: Data-centric AI, Memorability </li>
<li> Advised by <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>
</ul>

<b>Research Student at Data61-CSIRO & ANU (08.2020 - 05.2023)</b>
<ul>
<li> Topics: Generative models, Self-supervised learning </li>
<li> Advised by <a href="https://people.csiro.au/P/L/Lars-Petersson">Dr. Lars Petersson</a>, <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>
</ul>



<!-- define a function to add publication -->
<script type="text/javascript">
  function show_hide(eid) {
    var x = document.getElementById(eid);
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }

  // function add_publication(title, img, author, note, link, bib, tag){
  //   // link=[[url, arXiv],]
  //   document.write('<tr>')
  //   if (link!=null) {
  //     document.write(`<td><div style="width: 230px">\
  //     <a target="_blank" href=${link[0][0]}>\
  //     <img width="230" height="130" src=${img}></a>\
  //     </div></td>`)
  //   } else {
  //     document.write(`<td><div style="width: 230px">\
  //     <img width="230" height="130" src=${img}></a>\
  //     </div></td>`)
  //   }

  //   document.write('<td>')
  //   document.write(`<p class="title">${title}</p>`)
  //   document.write(`<p class="author">${author}</p>`)
  //   if (note) {document.write(`<p class="note">${note}</p>`)}
  //   if (link!=null) {
  //     document.write('<p class="link">')
  //     for (var idx = 0; idx < link.length; idx++){
  //       document.write(`<a target="_blank" href=${link[idx][0]}>[${link[idx][1]}]</a>&nbsp;`)}
  //     document.write('</p>')
  //   }
  //   document.write('</td></tr>')

  // }

  function add_publication(title, media, author, note, link, bib, tag){
    // link=[[url, arXiv],]
    document.write('<tr>')
    if (link!=null) {
      if(media.endsWith('.mp4')) {
        document.write(`<td><div style="width: 230px">\
        <a target="_blank" href=${link[0][0]}>\
        <video width="230" height="130" autoplay loop muted>\
          <source src=${media} type="video/mp4">\
          Your browser does not support the video tag.\
        </video></a>\
        </div></td>`)
      } else {
        document.write(`<td><div style="width: 230px">\
        <a target="_blank" href=${link[0][0]}>\
        <img width="230" height="130" src=${media}></a>\
        </div></td>`)
      }
    } else {
      if(media.endsWith('.mp4')) {
        document.write(`<td><div style="width: 230px">\
        <video width="230" height="130" autoplay loop muted>\
          <source src=${media} type="video/mp4">\
          Your browser does not support the video tag.\
        </video>\
        </div></td>`)
      } else {
        document.write(`<td><div style="width: 230px">\
        <img width="230" height="130" src=${media}>\
        </div></td>`)
      }
    }
    document.write('<td>')
    document.write(`<p class="title">${title}</p>`)
    document.write(`<p class="author">${author}</p>`)
    if (note) {document.write(`<p class="note">${note}</p>`)}
    if (link!=null) {
      document.write('<p class="link">')
      for (var idx = 0; idx < link.length; idx++){
        document.write(`<a target="_blank" href=${link[idx][0]}>[${link[idx][1]}]</a>&nbsp;`)}
      document.write('</p>')
    }
    document.write('</td></tr>')
  }
</script>


<A NAME="Publications"><h2>Publications</h2></A>
Representative papers (as leading author) are <font color="green">highlighted</font>.

<br>
<div>
  <button id="show-selected" style="padding: 10px 20px; background-color: #4c84af; color: white; border: none; border-radius: 4px; cursor: pointer; transition: background-color 0.3s;" onclick="showSelected()">Show Selected Papers</button>
  <button id="show-all" style="padding: 10px 20px; background-color: #4c84af; color: white; border: none; border-radius: 4px; cursor: pointer; transition: background-color 0.3s;" onclick="showAll()">Show All Papers</button>
</div>
<div id="selected-content" class="content">

  <script>
    function setInitialState() {
      document.getElementById('selected-content').style.display = 'block';
      document.getElementById('all-content').style.display = 'none';
    }

    document.addEventListener('DOMContentLoaded', setInitialState);

    document.getElementById('show-selected').addEventListener('click', function () {
      document.getElementById('selected-content').style.display = 'block';
      document.getElementById('all-content').style.display = 'none';
    });

    document.getElementById('show-all').addEventListener('click', function () {
      document.getElementById('selected-content').style.display = 'none';
      document.getElementById('all-content').style.display = 'block';
    });
    </script>

<table>
  <script type="text/javascript">
      add_publication(title='<font color="green">Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation</font>',
                      img='./projects/flex3d/videos/teaser/splash_video.mp4',
                      author='<b>Junlin Han</b>, Jianyuan Wang, Andrea Vedaldi, Philip Torr, Filippos Kokkinos',
                      note='Preprint',
                      link=[["https://junlinhan.github.io/projects/flex3d","Project page"], ["https://arxiv.org/abs/2410.00890","arXiv"], ["https://arxiv.org/pdf/2410.00890.pdf","pdf"], ["./bibtex/paper19.txt","BibTex"]]);
      add_publication(title='<font color="green">VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models</font>',
                      img='./paperimages/paper16.png',
                      author='<b>Junlin Han*</b>, Filippos Kokkinos*, Philip Torr',
                      note='ECCV, 2024',
                      link=[["https://junlinhan.github.io/projects/vfusion3d.html","Project page"], ["https://arxiv.org/abs/2403.12034","arXiv"], ["https://arxiv.org/pdf/2403.12034.pdf","pdf"], ["https://github.com/facebookresearch/vfusion3d","Code"], ["https://huggingface.co/spaces/facebook/VFusion3D", "Demo"], ["./Files/vfusion3d_poster.pdf","Poster"], ["./bibtex/paper16.txt","BibTex"]]);
      add_publication(title='<font color="green">What Images are More Memorable to Machines?</font>',
                   img='./paperimages/paper9.png',
                   author='<b>Junlin Han</b>, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, Ian Reid',
                   note='Preprint',
                   link=[["https://junlinhan.github.io/projects/machinemem.html","Project page"],["https://arxiv.org/abs/2211.07625","arXiv"], ["https://arxiv.org/pdf/2211.07625.pdf","pdf"], ["https://github.com/JunlinHan/MachineMem","Code"],["./bibtex/paper9.txt","BibTex"]]);
  add_publication(title='<font color="green">You Only Cut Once: Boosting Data Augmentation with a Single Cut </font>',
                   img='./paperimages/paper7.jpg',
                   author='<b>Junlin Han</b>, Pengfei Fang, Weihao Li, Jie Hong, Ali Armin, Ian Reid, Lars Petersson, Hongdong Li',
                   note='ICML, 2022',
                   link=[["https://arxiv.org/abs/2201.12078","arXiv"], ["https://arxiv.org/pdf/2201.12078.pdf","pdf"], ["https://proceedings.mlr.press/v162/han22a.html","PMLR"], ["https://github.com/JunlinHan/YOCO","Code"], ["./Files/YOCO_slide.pptx","Slide"], ["./bibtex/paper7.txt","BibTex"]]);
  add_publication(title='<font color="green">Blind Image Decomposition</font>',
                 img='./projects/resources/paper5/BID.gif',
                 author='<b>Junlin Han</b>, Weihao Li, Pengfei Fang, Chunyi Sun, Jie Hong, Ali Armin, Lars Petersson, Hongdong Li',
                 note='ECCV, 2022',
                 link=[["https://junlinhan.github.io/projects/BID.html","Project page"], ["https://arxiv.org/abs/2108.11364","arXiv"], ["https://arxiv.org/pdf/2108.11364.pdf","pdf"], ["https://github.com/JunlinHan/BID","Code"], ["https://github.com/JunlinHan/BID","Dataset"], ["https://youtu.be/wkyJDjUPCkg","Video"], ["./Files/BID_slide.pptx","Slide"], ["./Files/BID_poster.pdf","Poster"], ["./bibtex/paper5.txt","BibTex"]]);

  </script>
  </table>
</div>

<div id="all-content" class="content">
  <table>
    <script type="text/javascript">
      add_publication(title='VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models',
                      img='./paperimages/vgrp.png',
                      author='Yufan Ren, Konstantinos Tertikas, Shalini Maiti, <b>Junlin Han</b>, Tong Zhang, Sabine Süsstrunk, Filippos Kokkinos',
                      note='Preprint',
                      link=[["https://yufan-ren.com/subpage/VGRP-Bench/index.html","Project page"],["https://arxiv.org/abs/2503.23064","arXiv"], ["https://arxiv.org/pdf/2503.23064","pdf"], ["https://github.com/ryf1123/VGRP-Bench","Code"], ["https://huggingface.co/datasets/VGRP-Bench/VGRP-Bench","Dataset"],["./bibtex/vgrp.txt","BibTex"]]
                      );
      add_publication(title='Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model',
                      img='./paperimages/paper21.png',
                      author='Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, <b>Junlin Han</b>, Ender Konukoglu, Serge Belongie',
                      note='CVPR, 2025',
                      link=[["https://arxiv.org/abs/2503.16282","arXiv"], ["https://arxiv.org/pdf/2503.16282","pdf"], ["https://github.com/ZhaochongAn/GFS-VL","Code"],["./bibtex/paper21.txt","BibTex"]]
                      );
      add_publication(title='3D-GPT: Procedural 3D Modeling with Large Language Models',
                      img='./paperimages/paper13.png',
                      author='Chunyi Sun*, <b>Junlin Han*</b>, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould',
                      note='3DV, 2025',
                      link=[["https://chuny1.github.io/3DGPT/3dgpt.html","Project page"],["https://arxiv.org/abs/2310.12945","arXiv"], ["https://arxiv.org/pdf/2310.12945.pdf","pdf"], ["https://github.com/Chuny1/3DGPT","Code"],["./bibtex/paper13.txt","BibTex"]]
                      );
       add_publication(title='DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer',
                      img='./paperimages/paper18.jpg',
                      author='Runjia Li, <b>Junlin Han</b>, Luke Melas-Kyriazi, Chunyi Sun, Zhaochong An, Zhongrui Gui, Shuyang Sun, Philip Torr, Tomas Jakab',
                      note='3DV, 2025',
                      link=[["https://dreambeast3d.github.io/","Project page"],["https://arxiv.org/abs/2409.08271","arXiv"], ["https://arxiv.org/pdf/2409.08271","pdf"], ["https://github.com/runjiali-rl/threestudio-dreambeast","Code"],["./bibtex/paper18.txt","BibTex"]]
                      );
        add_publication(title='Semantic Score Distillation Sampling for Compositional Text-to-3D Generation',
                      img='./paperimages/paper20.jpg',
                      author='Ling Yang, Zixiang Zhang, <b>Junlin Han</b>, Bohan Zeng, Runjia Li, Philip Torr, Wentao Zhang',
                      note='Preprint',
                      link=[["https://arxiv.org/abs/2410.09009","arXiv"], ["https://arxiv.org/pdf/2410.09009.pdf","pdf"], ["https://github.com/YangLing0818/SemanticSDS-3D","Code"], ["./bibtex/paper20.txt","BibTex"]]);
        add_publication(title='<font color="green">Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation</font>',
                      img='./projects/flex3d/videos/teaser/splash_video.mp4',
                      author='<b>Junlin Han</b>, Jianyuan Wang, Andrea Vedaldi, Philip Torr, Filippos Kokkinos',
                      note='Preprint',
                      link=[["https://junlinhan.github.io/projects/flex3d","Project page"], ["https://arxiv.org/abs/2410.00890","arXiv"], ["https://arxiv.org/pdf/2410.00890.pdf","pdf"], ["./bibtex/paper19.txt","BibTex"]]);

      add_publication(title='Learning-based Multi-View Stereo: A Survey',
                      img='./paperimages/paper17.jpg',
                      author='Fangjinhua Wang*, Qingtian Zhu*, Di Chang*, Quankai Gao, <b>Junlin Han</b>, Tong Zhang, Richard Hartley, Marc Pollefeys',
                      note='Preprint',
                      link=[["https://arxiv.org/abs/2408.15235","arXiv"], ["https://arxiv.org/pdf/2408.15235","pdf"],["./bibtex/paper17.txt","BibTex"]]
                      );
      add_publication(title='<font color="green">VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models</font> ',
                      img='./paperimages/paper16.png',
                      author='<b>Junlin Han*</b>, Filippos Kokkinos*, Philip Torr',
                      note='ECCV, 2024',
                      link=[["https://junlinhan.github.io/projects/vfusion3d.html","Project page"], ["https://arxiv.org/abs/2403.12034","arXiv"], ["https://arxiv.org/pdf/2403.12034.pdf","pdf"], ["https://github.com/facebookresearch/vfusion3d","Code"], ["https://huggingface.co/spaces/facebook/VFusion3D", "Demo"], ["./bibtex/paper16.txt","BibTex"]]
                      );
      add_publication(title='Strong and Controllable Blind Image Decomposition',
                      img='./paperimages/paper15.png',
                      author='Zeyu Zhang*, <b>Junlin Han*</b>, Chenhui Gou*, Hongdong Li, Liang Zheng',
                      note='Preprint',
                      link=[["https://arxiv.org/abs/2403.10520","arXiv"], ["https://arxiv.org/pdf/2403.10520.pdf","pdf"], ["https://github.com/Zhangzeyu97/CBD.git","Code"],["./bibtex/paper15.txt","BibTex"]]
                      );
      add_publication(title='How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs',
                      img='./paperimages/paper14.png',
                      author='Haoqin Tu*, Chenhang Cui*, Zijun Wang*, Yiyang Zhou, Bingchen Zhao, <b>Junlin Han</b>, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie',
                      note='ECCV, 2024',
                      link=[["https://arxiv.org/abs/2311.16101","arXiv"], ["https://arxiv.org/pdf/2311.16101.pdf","pdf"], ["https://github.com/UCSC-VLAA/vllm-safety-benchmark","Code"],["./bibtex/paper14.txt","BibTex"]]
                      );
      add_publication(title=' NeRFEditor: Differentiable Style Decomposition for 3D Scene Editing',
                      img='./paperimages/paper10.png',
                      author='Chunyi Sun, Yanbin Liu, <b>Junlin Han</b>, Stephen Gould',
                      note='WACV, 2024',
                      link=[["https://chuny1.github.io/NeRFEditor/nerfeditor.html","Project page"],["https://arxiv.org/abs/2212.03848","arXiv"], ["https://arxiv.org/pdf/2212.03848.pdf","pdf"],["https://github.com/Chuny1/NeRFEditor","Code"],["./bibtex/paper10.txt","BibTex"]]
                      );
      add_publication(title='Hyperbolic Audio-visual Zero-shot Learning',
                      img='./paperimages/paper11.jpg',
                      author='Jie Hong, Zeeshan Hayder, <b>Junlin Han</b>, Pengfei Fang, Mehrtash Harandi, Lars Petersson',
                      note='ICCV, 2023',
                      link=[["https://arxiv.org/abs/2308.12558","arXiv"], ["https://arxiv.org/pdf/2308.12558.pdf","pdf"], ["https://github.com/JHome1/Hyper-AVZSL","Code"], ["./bibtex/paper11.txt","BibTex"] ]
                      );
     add_publication(title='Curved Geometric Networks for Visual Anomaly Recognition',
                  img='./paperimages/paper6.JPG',
                  author='Jie Hong, Pengfei Fang, Weihao Li, <b>Junlin Han</b>, Lars Petersson, Mehrtash Harandi',
                  note='TNNLS, 2023',
                  link=[["https://ieeexplore.ieee.org/abstract/document/10244211","Paper"],["https://arxiv.org/abs/2208.01188","arXiv"], ["https://arxiv.org/pdf/2208.01188.pdf","pdf"], ["https://github.com/JHome1/GiO-GiT","Code"], ["./bibtex/paper6.txt","BibTex"]]);
        add_publication(title='<font color="green"> What Images are More Memorable to Machines?</font>',
                     img='./paperimages/paper9.png',
                     author='<b>Junlin Han</b>, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, Ian Reid',
                     note='Preprint',
                     link=[["https://junlinhan.github.io/projects/machinemem.html","Project page"],["https://arxiv.org/abs/2211.07625","arXiv"], ["https://arxiv.org/pdf/2211.07625.pdf","pdf"], ["https://github.com/JunlinHan/MachineMem","Code"],["./bibtex/paper9.txt","BibTex"]]);
       add_publication(title='GOSS: Towards Generalized Open-set Semantic Segmentation',
                 img='./paperimages/paper3.png',
                 author='Jie Hong, Weihao Li, <b>Junlin Han</b>, Jiyang Zheng, Pengfei Fang, Mehrtash Harandi, Lars Petersson',
                 note='The Visual Computer, 2023',
                     link=[["https://link.springer.com/article/10.1007/s00371-023-02925-8","Paper"], ["https://arxiv.org/abs/2203.12116","arXiv"], ["https://arxiv.org/pdf/2203.12116.pdf","pdf"],["https://github.com/JHome1/GOSS_Segmentor","Code"], ["./bibtex/paper3.txt","BibTex"]]);
    add_publication(title='CropMix: Sampling a Rich Input Distribution via Multi-Scale Cropping',
                     img='./paperimages/paper8.png',
                     author='<b>Junlin Han</b>, Lars Petersson, Hongdong Li, Ian Reid',
                     note='Preprint',
                     link=[["https://arxiv.org/abs/2205.15955","arXiv"], ["https://arxiv.org/pdf/2205.15955.pdf","pdf"], ["https://github.com/JunlinHan/cropmix","Code"],["./bibtex/paper8.txt","BibTex"]]);
    add_publication(title='<font color="green"> You Only Cut Once: Boosting Data Augmentation with a Single Cut </font>',
                     img='./paperimages/paper7.jpg',
                     author='<b>Junlin Han</b>, Pengfei Fang, Weihao Li, Jie Hong, Ali Armin, Ian Reid, Lars Petersson, Hongdong Li',
                     note='ICML, 2022',
                     link=[["https://arxiv.org/abs/2201.12078","arXiv"], ["https://arxiv.org/pdf/2201.12078.pdf","pdf"], ["https://proceedings.mlr.press/v162/han22a.html","PMLR"], ["https://github.com/JunlinHan/YOCO","Code"], ["./Files/YOCO_slide.pptx","Slide"], ["./bibtex/paper7.txt","BibTex"]]);
    add_publication(title='<font color="green"> Blind Image Decomposition</font>',
                   img='./projects/resources/paper5/BID.gif',
                   author='<b>Junlin Han</b>, Weihao Li, Pengfei Fang, Chunyi Sun, Jie Hong, Ali Armin, Lars Petersson, Hongdong Li',
                   note='ECCV, 2022',
                   link=[["https://junlinhan.github.io/projects/BID.html","Project page"], ["https://arxiv.org/abs/2108.11364","arXiv"], ["https://arxiv.org/pdf/2108.11364.pdf","pdf"], ["https://github.com/JunlinHan/BID","Code"], ["https://github.com/JunlinHan/BID","Dataset"], ["https://youtu.be/wkyJDjUPCkg","Video"], ["./Files/BID_slide.pptx","Slide"], ["./Files/BID_poster.pdf","Poster"], ["./bibtex/paper5.txt","BibTex"]]);
    add_publication(title='Underwater Image Restoration via Contrastive Learning and a Real-world Dataset',
                   img='./paperimages/paper4.JPG',
                   author='<b>Junlin Han</b>, Mehrdad Shoeiby, Tim Malthus, Elizabeth Botha, Janet Anstee,  Saeed Anwar, Ran Wei, Ali Armin, Hongdong Li, Lars Petersson',
                   note='Remote Sensing (best paper award), 2022',
                   link=[["https://www.mdpi.com/2072-4292/14/17/4297","Paper"], ["https://github.com/JunlinHan/CWR","Code"],["https://github.com/JunlinHan/CWR","Dataset"],["./bibtex/paper4.txt","BibTex"]]);
      add_publication(title='Single Underwater Image Restoration by Contrastive Learning',
                      img='./paperimages/paper2.JPG',
                      author='<b>Junlin Han</b>, Mehrdad Shoeiby, Tim Malthus, Elizabeth Botha, Janet Anstee, Saeed Anwar, Ran Wei, Lars Petersson, Ali Armin',
                      note='IGARSS (oral), 2021',
                      link=[["https://arxiv.org/abs/2103.09697","arXiv"], ["https://arxiv.org/pdf/2103.09697.pdf","pdf"],
                      ["https://github.com/JunlinHan/CWR","Code"],["https://github.com/JunlinHan/CWR","Dataset"]
                      ,["./bibtex/paper2.txt","BibTex"]]);
     add_publication(title='Dual Contrastive Learning for Unsupervised Image-to-Image Translation',
                      img='./paperimages/paper1.PNG',
                      author='<b>Junlin Han</b>, Mehrdad Shoeiby, Lars Petersson, Ali Armin',
                        note=' NTIRE, CVPRW (oral), 2021',
                      link=[["https://arxiv.org/abs/2104.07689","arXiv"], ["https://arxiv.org/pdf/2104.07689.pdf","pdf"], ["https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Han_Dual_Contrastive_Learning_for_Unsupervised_Image-to-Image_Translation_CVPRW_2021_paper.pdf","CVF"],
                      ["https://github.com/JunlinHan/DCLGAN","Code"],["https://youtu.be/w0oltXvLgmI","Video"],["./Files/DCLGAN_slide.pptx","Slide"],["./bibtex/paper1.txt","BibTex"]]);
    </script>
    </table>
  </div>
<p><br /></p>

<!-- <A NAME="Talks"><h2>Talks</h2></A>
<b>I share some of my talks and slides here, please feel free to use them if you are interested in.</b>
<ul>
<li><b> Image synthesis</b> </li>
<a href="./Files/StyleGAN.pptx">StyleGAN</a>
<li><b> Image-to-Image Translation</b> </li>
<a href="./Files/DCLGAN_slide.pptx">DCLGAN</a>
</ul> -->

<A NAME="Services"><h2>Academic Services</h2></A>
<ul>
<li><b>Workshop Organization</b></li>
2nd Workshop on Foundation Models in the Wild at ICLR 2025
<br>
Foundation Models in the Wild at ICML 2024
<br>
<li><b>Conference review</b></li>
CVPR 2022 2023 2024 2025, ICCV 2023, ECCV 2022 2024, ICML 2022 2024 2025, NeurIPS 2022 2023, ICLR 2023 2024 2025, SIGGRAPH Asia 2024, ACCV 2022, BMVC 2023, WACV 2024, AAAI 2023
<br>
<li><b>Journal review</b> </li>
Transactions on Pattern Analysis and Machine Intelligence (TPAMI),  International Journal of Computer Vision (IJCV), Transactions on Image Processing (TIP), Transactions on Geoscience and Remote Sensing (TGRS)
</ul>
</font>

<A NAME="Skills"><h2>Skills</h2></A>
<ul>
<li><b>Programming Language</b></li>
Proficient in MATLAB, Python
<br>
Familiar with Java, Haskell, R, CSS, Html, Assembly (ARMV7), SQL
<Li><b>Tool</b></li>
NumPy, PyTorch, Git, LaTeX
<li><b>Language</b></li>
English, Singlish, and Madarian
</ul>
</font>

<A NAME="Awards"><h2>Awards</h2></A>
<ul>
  <li><b>Meta PhD Scholarship, 2023</b></br>
    for PhD research at University of Oxford
    </li>
    <li><b>Best paper award, 2022</b></br>
      from Remote Sensing journal
      </li>
    <li><b>Chancellor's Letter of Commendation, 2022</b></br>
      for best academic performance at Australian National University
  </li>
  <li><b>Top Reviewer, 2022</b></br>
    NeurIPS (Conference on Neural Information Processing Systems)
</li>
<li><b>Second Best Presentation Award, 2021</b></br>
  AIM (Active Integrated Matter) Conference
  </li>
<li><b>Top-up Scholarship, 2021</b></br>
for research work at Data61-CSIRO
</li>
<li><b>Undergraduate Vacation Scholarship, 2020</b></br>
for summer research at Data61-CSIRO
</li>
</ul>
</font>

<A NAME="Miscs"><h2>Miscs</h2></A>
<ul>
<li>At the beginning, I was born and raised in <a href="https://en.wikipedia.org/wiki/Chengdu">Chengdu</a>, which is famous for spice and pandas. </li>
<li>I studied and lived in China, Singapore, Australia, and UK.</li>
<li>Aside from research, I am passionate about traveling (especially nature travel), cooking, gaming, and swimming.   </li>
</ul>
</font>

</br>
</br>

<table class="sub-table" style="width: 200px;height: 100px;" align="center">
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=A_M689vS_rmlsTbtLHxcdi_-Ti8JbhM0uQ_c6itmJb8&cl=ffffff&w=a"></script>
  </table>

<!-- <a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?junlin"
border="0" alt="Web Counter"></a>
<br><a href="https://www.easycounter.com/">unique visitors since April 2021</a> -->


</body>
</html>

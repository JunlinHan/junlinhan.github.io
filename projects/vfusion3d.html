<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size:32px;
		font-weight:300;
	}

	h2 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models</span>
		<table align=center width=900px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://junlinhan.github.io/">Junlin Han</a><sup>1,2*</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://fkokkinos.github.io/">Filippos Kokkinos</a><sup>1*</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.robots.ox.ac.uk/~phst/">Philip Torr</a><sup>2</sup></span>
						</center>
					</td>
				</tr>
			</table>

			<table align=center width=900px>
				<tr>
					<td align=center width=900px>
						<center>
							<span style="font-size:24px"><sup>1</sup> GenAI, Meta  &emsp; <sup>2</sup> TVG, University of Oxford</a></span>
						</center>
					</td>
				</tr>
			</table>


			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/pdf/2403.12034.pdf'>[paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2403.12034'>[arXiv]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/facebookresearch/vfusion3d'>[code]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://huggingface.co/spaces/facebook/VFusion3D'>[demo]</a></span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<table align=center width=900px>
		<tr>
			<td align=center width=900px>
				<center>
					<span style="font-size:24px">ECCV 2024</span>
				</center>
			</td>
		</tr>
	</table>
</center>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data.
				To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time.
			</td>
		</tr>
	</table>
	<br>

	<center><h1>Overall pipeline</h1></center>
		<table align=center width=1000px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:1000px" src="./resources/paper16/overall.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<table align=center width=850px>
		<tr>
			<td>
		     <b>The pipeline of VFusion3D.</b>  We first use a small amount of 3D data to
			 fine-tune a video diffusion model, transforming it into a multi-view vodeo generator
			 that functions as a data engine. By generating a large amount of synthetic data, we
			 train VFusion3D to generate a 3D representation and render novel views.
			</td>
		</tr>
	</table>
	<br>

	<center><h1>Results</h1></center>
	<table align=center width=1000px>
		<tr>
			<td width=260px>
				<center>
					<h2>Generated Images (Text-Image-3D)</h2>
					<img class="round" style="width:1000px" src="./resources/paper16/gif1.gif"/>
				</center>
			</td>
		</tr>
	</table>
	<br></br>
	<table align=center width=1000px>
		<tr>
			<td width=260px>
				<center>
					<h2>Single Image 3D Reconstruction</h2>
					<img class="round" style="width:1000px" src="./resources/paper16/gif2.gif"/>
				</center>
			</td>
		</tr>
	</table>
	<br></br>
	<table align=center width=1000px>
		<tr>
			<td width=260px>
				<center>
					<h2>User Study</h2>
					<img class="round" style="width:1000px" src="./resources/paper16/user.png"/>
				</center>
			</td>
		</tr>
	</table>
</center>



<center><h1>Scaling!</h1></center>
<table align=center width=1000px>
	<tr>
		<td width=260px>
			<center>
				<h2>Scaling with the number of synthetic data</h2>
				<img class="round" style="width:1000px" src="./resources/paper16/scale.png"/>
			</center>
		</td>
	</tr>
</table>
</center>

<table align=center width=850px>
	<tr>
		<td>
		The left and right figures display the LPIPS and CLIP image similarity scores in relation to the dataset size,
		 respectively. The generation quality consistently improves as the dataset size increases
		</td>
	</tr>
</table>

<br></br>
<table align=center width=850px>
	<tr>
		<td>		<center><h2>Scaling with other factors</h2></center>
			
			Our approach can also scale and improve with several other factors. These include the <b>development of stronger video diffusion models</b>, the availability of  <b>more 3D data for fine-tuning</b> the video diffusion model and the pre-trained 3D generative model, and the <b>advancement of large 3D feed-forward generative models</b>. All these factors contribute to the scalability of our model, positioning it as a promising avenue for foundation 3D generative models.
		</td>
	</tr>
</table>


<br>



	
	<table align=center width=450px>
		<center><h1>Paper</h1></center>
		<tr>
			<td><a href="https://arxiv.org/pdf/2403.12034.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Han <i>et al.</i><br>
				<b>VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models</b><br>
				<a href="https://arxiv.org/abs/2403.12034">preprint </a><br>
				(hosted on <a href="https://arxiv.org/abs/2403.12034">ArXiv</a>)<br>
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/paper16/paper16.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					Junlin Han is supported by Meta. We would like to thank Jianyuan Wang, Luke Melas-Kyriazi, Yawar Siddiqui, Quankai Gao, Yanir Kleiman, Roman Shapovalov, Natalia Neverova, and Andrea Vedaldi for the insightful discussions and invaluable support.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

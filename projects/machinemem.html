<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size:32px;
		font-weight:300;
	}

	h2 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>What Images are More Memorable to Machines?</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">What Images are More Memorable to Machines?</span>
		<table align=center width=900px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://junlinhan.github.io/">Junlin Han</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://huangying-zhan.github.io/">Huangying Zhan</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px">Jie Hong</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px">Pengfei Fang</span>
						</center>
					</td>
				</tr>
			</table>

			<table align=center width=900px>
			<tr>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a></span>
					</center>
				</td>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://people.csiro.au/P/L/Lars-Petersson">Lars Petersson</a></span>
					</center>
				</td>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://cs.adelaide.edu.au/~ianr/">Ian Reid</a></span>
					</center>
				</td>
			</tr>
			</table>

			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/pdf/2211.07625.pdf'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2211.07625'>[arXiv]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/JunlinHan/MachineMem'>[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>


	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				This paper studies the problem of measuring and predicting how memorable an image is to pattern recognition machines, as a path to explore machine intelligence. Firstly, we propose a self-supervised machine memory quantification pipeline, dubbed ``MachineMem measurer'', to collect machine memorability scores of images. Similar to humans, machines also tend to memorize certain kinds of images, whereas the types of images that machines and humans memorialize are different. Through in-depth analysis and comprehensive visualizations, we gradually unveil that "complex" images are usually more memorable to machines. We further conduct extensive experiments across 11 different machines (from linear classifiers to modern ViTs) and 9 pre-training methods to analyze and understand machine memory. This work proposes the concept of machine memorability and opens a new research direction at the interface between machine memory and visual data.  
			</td>
		</tr>
	</table>
	<br>

	<center><h1>Measure Machine Memory</h1></center>
		<table align=center width=1000px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:1000px" src="./resources/paper9/ppline_set.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<table align=center width=850px>
		<tr>
			<td>
		     <b>A realization of the MachineMem measurer.</b> Our MachineMem measurer has three stages: (a) Seeing images through self-supervision, (b) Recognizing seen and unseen images, and (c) Predicting whether an image has been seen. Each image presented here (cat, apple, and bird) denotes an 
			 image set (<i>A, B,</i> and <i>C</i>) containing n images. We are interested in measuring MachineMem scores of set <i>A</i>. 
			 In each episode of the MachineMem measurer, we randomly draw set <i>B</i> and set <i>C</i> from a large dataset while keeping the cat set identical. 
			 MachineMem scores of set <i>A</i> are obtained by repeating MachineMem measurer <i>m</i> times. 
			</td>
		</tr>

		<tr>
			<td>
			 <B>Design idea:</B> The design of the MachineMem measurer follows the key idea used in the <a href="http://web.mit.edu/phillipi/Public/MemorabilityPAMI/index.html">visual memory game</a>, that is, quantifying machine memory through a repeat detection task. The visual memory game can be summarized using 3 keywords: see, repeat, and detect. We design the MachineMem measurer as a 3-stage pipeline, where each stage corresponds to a keyword.  
			</td>
		</tr>
	</table>
	<br>

	<center><h1>Predict Memorability Scores</h1></center>
<table align=center width=850px>
	<tr>
		<td>
			Collecting MachineMem scores with the MachineMem measurer can be time-consuming. Thereby, we trained our MachineMem predictor and HumanMem predictor. 
			<br><B>Upload your image to see how memorable it is!</B> 
		<br>
		</td>
	</tr>
</table>
<br>
<script type="module"
src="https://gradio.s3-us-west-2.amazonaws.com/3.9.1/gradio.js">
</script>
<gradio-app space="Junlinh/memorability_prediction"></gradio-app>


<center><h1>What Makes an Image Memorable to Machines?</h1></center>
<center><h2>Quantitative analysis</h2></center>
<table align=center width=850px>
	<tr>
		<td>
			Are image attributes enough to determine whether images will be memorable to machines? We study 13 image attributes. 
		</td>
	</tr>
</table>
<table align=center width=1000px>
	<tr>
		<td width=260px>
			<center>
				<img class="round" style="width:1000px" src="./resources/paper9/property.png"
		</td>
	</tr>
</table>
<table align=center width=850px>
	<tr>
		<td>
			We sort all LaMem images and group them into 10 groups, from the group with a mean of lowest MachineMem scores to the highest. Value and contrast are the two most notable attributes that correlate moderately (<i>ρ</i> ≥ 0.3) with MachineMem scores. Spearman’s correlation (<i>ρ</i>) is computed based on all data (58741 <a href="http://memorability.csail.mit.edu/index.html">laMem</a> images).
		</td>
	</tr>
</table>


<center><h2>What classes are more or less memorable?</h2></center>
<table align=center width=850px>
	<tr>
		<td>
			Do images belonging to certain classes tend to be more or less memorable to machines? We use the MachineMem predictor to predict MachineMem scores of all ImageNet training images to obtain mean MachineMem scores of 1000 (ImageNet) classes. 
		</td>
	</tr>
</table>
<table align=center width=1000px>
	<tr>
		<td width=260px>
			<center>
				<img class="round" style="width:1000px" src="./resources/paper9/class.png"
		</td>
	</tr>
</table>
<table align=center width=850px>
	<tr>
		<td>
			 We report the top-5 and bot-5 classes and their mean MachineMem scores. The top classes tend to have lower value and stronger contrast.  The bottom classes usually have light backgrounds covering a large percentage of pixels. 
		</td>
	</tr>
</table>


<center><h2>GANalyze</h2></center>
<table align=center width=850px>
	<tr>
		<td>
			Some hidden factors that may make an image more memorable to machines are still veiled. Thereby, we utilize the power of GANalyze to discover hidden factors that might determine MachineMem scores. 
		</td>
	</tr>
</table>
<table align=center width=1000px>
	<tr>
		<td width=260px>
			<center>
				<img class="round" style="width:1000px" src="./resources/paper9/ganalyze.png"
		</td>
	</tr>
</table>
<table align=center width=850px>
	<tr>
		<td>
We summarize 6 trends, where the first 3 of them (value, contrast, and number of objects) are previously shown. For certain objects, viewing angle, shape, and tasty are hidden trends that
are unveiled by GANalyze. An overall trend is that GANalyze is often complexifying images to make them more memorable to machines.
		</td>
	</tr>
</table>

<center><h2>Human memory vs. machine memory </h2></center>
<table align=center width=850px>
	<tr>
		<td>
			MachineMem scores and HumanMem scores are very weakly correlated. But in GANalyze, which is good at showing global trends, we find machines tend to memorize more complex images, which is on the reverse side of humans that are usually better at memorializing simple images.
		</td>
	</tr>
</table>
<table align=center width=1000px>
	<tr>
		<td width=260px>
			<center>
				<img class="round" style="width:1000px" src="./resources/paper9/compare.png"
		</td>
	</tr>
</table>
<table align=center width=850px>
	<tr>
		<td>
			HumanMem scores are labeled in <font color="red"> red </font> while <font color="blue"> blue </font> indicates MahinceMem scores. Generally speaking, simple
			images are more memorable to humans while complex images are more memorable to machines.
		</td>
	</tr>
</table>


<center><h1>Understanding Machine Memory</h1></center>
<table align=center width=850px>
	<tr>
		<td>
			HumanMem is an intrinsic and stable property of an image that is shared across different humans, that is, though humans have dissimilar backgrounds, their memory regarding visual data is similar. So how about machines?
<br>
Here we explore two questions to shed light on understanding machine memory: Will MachineMem scores keep consistent across different machines (<b>left</b>)? What is the role of varied pre-training knowledge (<b>right</b>)?
		</td>
	</tr>
</table>
<table align=center width=1000px>
	<tr>
		<td width=260px>
			<center>
				<img class="round" style="width:1000px" src="./resources/paper9/understand.png"
		</td>
	</tr>
</table>
<table align=center width=850px>
	<tr>
		<td>
			Spearman's correlation (<i>ρ</i>) of two machines/methods is presented at each off-diagonal. 
			<br>
			<b>left:</b> Machines within each category (conventional machines, classic CNNs, modern CNNs, and recent ViTs) usually show strong correlations. However, machines across each category do not show obvious correlations most of the time.
			<br>
			<b>right:</b> For an identical structured machine, MachineMem can be treated as an intrinsic and stable property of an image that is shared across different backgrounds (pre-training knowledge). 
		</td>
	</tr>
</table>


	<!-- <hr>
	<center><h1>Talk</h1></center>
	Coming soon.
	<p align="center">
		<iframe width="660" height="395" src="https://youtu.be/wkyJDjUPCkg" frameborder="10" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p> -->

	
	<table align=center width=450px>
		<center><h1>Paper</h1></center>
		<tr>
			<td><a href="https://arxiv.org/pdf/2211.07625.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Han <i>et al.</i><br>
				<b>What Images are More Memorable to Machines?</b><br>
				<a href="https://arxiv.org/abs/2211.07625">preprint </a><br>
				(hosted on <a href="https://arxiv.org/abs/2211.07625">ArXiv</a>)<br>
				(<a href="https://github.com/JunlinHan/MachineMem">Code</a>)<br>
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/paper9/paper9.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>
